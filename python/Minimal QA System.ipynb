{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf52bc3",
   "metadata": {},
   "source": [
    "Start docker container with elasticsearch:\n",
    "_docker pull docker.elastic.co/elasticsearch/elasticsearch:7.12.1\n",
    "_docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f64bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2021 22:08:25 - INFO - faiss.loader -   Loading faiss with AVX2 support.\n",
      "05/04/2021 22:08:25 - INFO - faiss.loader -   Loading faiss.\n",
      "/home/daniel/anaconda3/envs/KD/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "05/04/2021 22:08:26 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from TEI_Handling import TEIFile\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import Pool\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.preprocessor import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606338bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2021 22:08:28 - INFO - elasticsearch -   HEAD http://localhost:9200/ [status:200 request:0.007s]\n",
      "05/04/2021 22:08:28 - INFO - elasticsearch -   HEAD http://localhost:9200/document [status:200 request:0.002s]\n",
      "05/04/2021 22:08:28 - INFO - elasticsearch -   GET http://localhost:9200/document [status:200 request:0.001s]\n",
      "05/04/2021 22:08:28 - INFO - elasticsearch -   PUT http://localhost:9200/document/_mapping [status:200 request:0.010s]\n",
      "05/04/2021 22:08:28 - INFO - elasticsearch -   HEAD http://localhost:9200/label [status:200 request:0.002s]\n"
     ]
    }
   ],
   "source": [
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6443870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(paper_path):\n",
    "    tei = TEIFile(paper_path)\n",
    "    processor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=250,\n",
    "        split_respect_sentence_boundary=True,\n",
    "        split_overlap=20\n",
    "    )\n",
    "    return processor.process(tei.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = sorted(Path(\"../data/unpaywall-grobid-sample\").glob('*.tei.xml'))\n",
    "print(f\"Processing {papers.__len__()} papers on {multiprocessing.cpu_count()} cores.\")\n",
    "pool = Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = []\n",
    "dicts.extend(pool.imap(to_dict, papers, 5))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4de8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for inner_list in dicts:\n",
    "    documents.extend(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b89392",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe59079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0372bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2021 22:08:48 - INFO - farm.utils -   Using device: CPU \n",
      "05/04/2021 22:08:48 - INFO - farm.utils -   Number of GPUs: 0\n",
      "05/04/2021 22:08:48 - INFO - farm.utils -   Distributed Training: False\n",
      "05/04/2021 22:08:48 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "05/04/2021 22:08:58 - WARNING - farm.utils -   ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "05/04/2021 22:08:58 - INFO - farm.utils -   Using device: CPU \n",
      "05/04/2021 22:08:58 - INFO - farm.utils -   Number of GPUs: 0\n",
      "05/04/2021 22:08:58 - INFO - farm.utils -   Distributed Training: False\n",
      "05/04/2021 22:08:58 - INFO - farm.utils -   Automatic Mixed Precision: None\n",
      "05/04/2021 22:08:59 - INFO - farm.infer -   Got ya 11 parallel workers to do inference ...\n",
      "05/04/2021 22:08:59 - INFO - farm.infer -    0    0    0    0    0    0    0    0    0    0    0 \n",
      "05/04/2021 22:08:59 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\\n",
      "05/04/2021 22:08:59 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\\n",
      "05/04/2021 22:08:59 - INFO - farm.infer -                       \n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:19 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:56 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:56 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:56 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:56 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n",
      "05/04/2021 22:09:57 - WARNING - farm.data_handler.dataset -   Could not determine type for feature 'labels'. Converting now to a tensor of default type long.\n"
     ]
    }
   ],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadd2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipeline import ExtractiveQAPipeline\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a939317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2021 22:09:56 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.021s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.20 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.09 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.36 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.16 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.17 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.51 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.49 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipe.run(query=\"What is the IETF?\", top_k_retriever=10, top_k_reader=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7862abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb1f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'answer': 'Internet Engineering Task Force',\n",
      "        'context': 'ibed in the Requests For Comment (RFC) documents [2] of '\n",
      "                   'the Internet Engineering Task Force (or IETF) [3]. RFCs '\n",
      "                   'are numbered when they are accepted to'},\n",
      "    {   'answer': '= ( , , Σ, , , , , ) in initial configuration . Input '\n",
      "                  'stimuli move the APN',\n",
      "        'context': 'scheme plus adaptive layer as follows:\\n'\n",
      "                   '= ( , , Σ, , , , , ) in initial configuration . Input '\n",
      "                   'stimuli move the APN to the next configuration if, and on'},\n",
      "    {   'answer': 'Σ',\n",
      "        'context': 'al behavior in step k; ε (\"empty string\") denotes absence '\n",
      "                   'of valid element;\\n'\n",
      "                   'Σ is the set of all possible events that make up the input '\n",
      "                   'chain; A⊆C is t'},\n",
      "    {   'answer': 'Σ',\n",
      "        'context': 'al behavior in step k; ε (\"empty string\") denotes absence '\n",
      "                   'of valid element;\\n'\n",
      "                   'Σ is the set of all possible events that make up the input '\n",
      "                   'chain; A⊆C is t'},\n",
      "    {   'answer': 'some kind of processing',\n",
      "        'context': 'Each layer is in charge of some kind of processing and '\n",
      "                   'each layer only talks to the layers immediately below and '\n",
      "                   'above it. One given layer receives da'}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd9592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
