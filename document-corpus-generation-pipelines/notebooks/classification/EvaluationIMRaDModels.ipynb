{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EvaluationIMRaDModels.ipynb",
      "provenance": [],
      "mount_file_id": "1gAzx4TtvCtA5SYdwmBoHiF46OtZ1DM9Q",
      "authorship_tag": "ABX9TyPh6frd60KQkrmVVQdl1ghg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-gomm/document-corpus-generation-pipelines/blob/main/document-corpus-generation-pipelines/notebooks/classification/EvaluationIMRaDModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwkxsctX82TS"
      },
      "source": [
        "Script where the IMRaD models are evaluated. Probably best to open this in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYZNbep64rQG",
        "outputId": "1e5202b3-6ec9-40ae-af04-3411e6d921dc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 48.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxHMNoy84oh2"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoModelForSequenceClassification \n",
        "\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertModel, BertConfig\n",
        "import random\n",
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "import nltk\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV5KsM2H1jWX",
        "outputId": "57b656d1-8915-431d-b417-f21695741f71"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyNt-khqloD4"
      },
      "source": [
        "def evaluateIMRADFromJson(data, modelstring, modelname, finetunedModelPath,tokenizerName, roberta, cased): #params: data: evaluation data, modelstring: base model string to be used, modelName:modelName, toenizerName, roberta: whether roberta is used, cased: whether to use a cased tokenizer-->True:lower cased tokenizer\n",
        "  label_dict = {}\n",
        "  label_dict[\"intro\"]=0\n",
        "  label_dict[\"related\"]=1\n",
        "  label_dict[\"method\"]=2\n",
        "  label_dict[\"results\"]=3\n",
        "  label_dict[\"discussion\"]=4\n",
        "  #load tokenizer\n",
        "  tokenizerString=modelstring\n",
        "  if (roberta==True):\n",
        "    tokenizer = tokenizerName.from_pretrained(tokenizerString)\n",
        "    print(\"roberta\")\n",
        "  else: \n",
        "    tokenizer = tokenizerName.from_pretrained(tokenizerString,do_lower_case=cased)\n",
        "    #print(cased)\n",
        "  #load model, create pipeline which returns LABEL_0 to LABEL_4 based on section\n",
        "  model = modelname.from_pretrained(modelstring, num_labels=5, output_attentions=False, output_hidden_states=False)\n",
        "  model.load_state_dict(torch.load(finetunedModelPath, map_location=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')))        \n",
        "  classificationPipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
        "  c = 5\n",
        "  r = 5\n",
        "  #Evaluation Array: Which catgeory was labelled as what?\n",
        "  EvaluationArray = [ [0] * c for i in range(r) ]\n",
        "  EvaluationArrayWithContext = [ [0] * c for i in range(r) ]\n",
        "  EvaluationArrayWithSections= [ [0] * c for i in range(r) ]\n",
        "  EvaluationArrayWithSectionsAfterContext= [ [0] * c for i in range(r) ]\n",
        "  arrayLabels=[]\n",
        "  arrayBase=[]\n",
        "  arrayContext=[]\n",
        "  arraySection=[]\n",
        "  labelArray=[]\n",
        "\n",
        "  counterForSections=0\n",
        "  rightLabels=0\n",
        "  for paperCounter in range(1,(len(data))):\n",
        "    predictedLabels=[]\n",
        "    predictedLabels2=[]\n",
        "    predictedLabels3=[]\n",
        "\n",
        "    helperArray=[]\n",
        "    arrayLabels.append(labelArray)\n",
        "    labelArray=[]\n",
        "    idArray=[]\n",
        "    sections=data[str(paperCounter)]\n",
        "\n",
        "    if (bool(data[str(paperCounter)]))==True:\n",
        "      for sectionCounter in range(0,len(sections)):\n",
        "        if (len(sections[str(sectionCounter)][\"text\"].split())>150):\n",
        "          print(\"error\")\n",
        "          sections[str(sectionCounter)][\"text\"]=\"error\"\n",
        "          continue\n",
        "        \n",
        "        predictedLabels.append(classificationPipeline(sections[str(sectionCounter)][\"text\"])[0][\"label\"]) #predict label\n",
        "        labelArray.append(sections[str(sectionCounter)][\"section\"])\n",
        "\n",
        "      for i in range(0,len(predictedLabels)):\n",
        "\n",
        "        int1=0\n",
        "        int2=0\n",
        "        for key in label_dict:\n",
        "          if (labelArray[i]==key):\n",
        "            int1=label_dict[key]\n",
        "        label=predictedLabels[i]\n",
        "        #replace label with matching string, more intuitive\n",
        "        if (label==\"LABEL_0\"):\n",
        "          label=\"intro\"     \n",
        "        if (label==\"LABEL_1\"):\n",
        "          label=\"related\"            \n",
        "        if (label==\"LABEL_2\"):\n",
        "          label=\"method\"           \n",
        "        if (label==\"LABEL_3\"):\n",
        "          label=\"results\"           \n",
        "        if (label==\"LABEL_4\"):\n",
        "          label=\"discussion\"\n",
        "        predictedLabels[i]=label\n",
        "        for key in label_dict:\n",
        "          if (label==key):\n",
        "            int2=label_dict[key]       \n",
        "        EvaluationArray[int1][int2]=int(EvaluationArray[int1][int2])+1\n",
        "      predictedLabels2=predictedLabels[:]\n",
        "      predictedLabels3=predictedLabels[:]\n",
        "\n",
        "      arrayBase.append(predictedLabels)\n",
        "      for i in range(0,len(predictedLabels2)):\n",
        "        if (i<(len(predictedLabels2)-2)):\n",
        "          if (predictedLabels2[i]==predictedLabels2[i+2]):\n",
        "            predictedLabels2[i+1]=predictedLabels2[i]\n",
        "\n",
        "          int1=0\n",
        "          int2=0\n",
        "\n",
        "          for key in label_dict:\n",
        "            if (labelArray[i]==key):\n",
        "              int1=label_dict[key]\n",
        "          label=predictedLabels2[i]\n",
        "        for key in label_dict:\n",
        "          if (label==key):\n",
        "            int2=label_dict[key]       \n",
        "        EvaluationArrayWithContext[int1][int2]=int(EvaluationArrayWithContext[int1][int2])+1\n",
        "      arrayContext.append(predictedLabels2)\n",
        "\n",
        "      most_common = max(predictedLabels, key = predictedLabels.count)\n",
        "      for i in range(0,len(predictedLabels)):\n",
        "        predictedLabels3[i]=most_common\n",
        "        for key in label_dict:\n",
        "          if (labelArray[i]==key):\n",
        "            int1=label_dict[key]\n",
        "        label=predictedLabels3[i]\n",
        "        for key in label_dict:\n",
        "          if (label==key):\n",
        "            int2=label_dict[key]       \n",
        "        EvaluationArrayWithSections[int1][int2]=int(EvaluationArrayWithSections[int1][int2])+1\n",
        "\n",
        "      arraySection.append(predictedLabels3)\n",
        "\n",
        "\n",
        "  rightLabels=EvaluationArray[0][0]+EvaluationArray[1][1]+EvaluationArray[2][2]+EvaluationArray[3][3]+EvaluationArray[4][4]\n",
        "  rightLabelsIntroRelated=EvaluationArray[0][0]+EvaluationArray[0][1]+EvaluationArray[1][0]+EvaluationArray[1][1]+EvaluationArray[2][2]+EvaluationArray[3][3]+EvaluationArray[4][4]\n",
        "  for i in EvaluationArray:\n",
        "    for p in i:\n",
        "      sectionCounter+=p\n",
        "  #print statements give further information which categroy was labelled as what\n",
        "  #print(f\"Without Context:{rightLabels/sectionCounter}\")\n",
        "  #print(f\"Without Context intro=related:{rightLabelsIntroRelated/sectionCounter}\")\n",
        "  #print(EvaluationArray)\n",
        "\n",
        "  rightLabels=EvaluationArrayWithContext[0][0]+EvaluationArrayWithContext[1][1]+EvaluationArrayWithContext[2][2]+EvaluationArrayWithContext[3][3]+EvaluationArrayWithContext[4][4]\n",
        "  rightLabelsIntroRelated=EvaluationArrayWithContext[0][0]+EvaluationArrayWithContext[0][1]+EvaluationArrayWithContext[1][0]+EvaluationArrayWithContext[1][1]+EvaluationArrayWithContext[2][2]+EvaluationArrayWithContext[3][3]+EvaluationArrayWithContext[4][4]\n",
        "  sectionCounter=0\n",
        "  for i in EvaluationArrayWithContext:\n",
        "    for p in i:\n",
        "      sectionCounter+=p\n",
        "  #print(f\"With Context:{rightLabels/sectionCounter}\")\n",
        "  #print(f\"With Context, intro=related:{rightLabelsIntroRelated/sectionCounter}\")\n",
        "  #print(EvaluationArrayWithContext)\n",
        "\n",
        "  rightLabels=EvaluationArrayWithSections[0][0]+EvaluationArrayWithSections[1][1]+EvaluationArrayWithSections[2][2]+EvaluationArrayWithSections[3][3]+EvaluationArrayWithSections[4][4]\n",
        "  rightLabelsIntroRelated=EvaluationArrayWithSections[0][0]+EvaluationArrayWithSections[0][1]+EvaluationArrayWithSections[1][0]+EvaluationArrayWithSections[1][1]+EvaluationArrayWithSections[2][2]+EvaluationArrayWithSections[3][3]+EvaluationArrayWithSections[4][4]\n",
        "  sectionCounter=0\n",
        "  for i in EvaluationArrayWithSections:\n",
        "    for p in i:\n",
        "      sectionCounter+=p\n",
        "  #print(f\"With Sections:{rightLabels/sectionCounter}\")\n",
        "  #print(f\"With Sections, intro=related:{rightLabelsIntroRelated/sectionCounter}\")\n",
        "  #print(EvaluationArrayWithSections)\n",
        "\n",
        "  arrayLabels.append(labelArray)\n",
        "  array=[arrayLabels,arrayBase,arrayContext,arraySection]\n",
        "  return (array)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVuRdZbyirpV"
      },
      "source": [
        "BertTokenizerName=BertTokenizer\n",
        "\n",
        "BertStringUncased=\"bert-base-uncased\"\n",
        "BertStringCased=\"bert-base-cased\"\n",
        "\n",
        "ScibertStringUncased=\"allenai/scibert_scivocab_uncased\"\n",
        "ScibertStringCased=\"allenai/scibert_scivocab_cased\"\n",
        "\n",
        "\n",
        "BertModelName=BertForSequenceClassification\n",
        "ScibertPath=\"allenai/scibert_scivocab_cased\"\n",
        "\n",
        "adressToSaveModelBertUncased=\"BertUncased\"\n",
        "adressToSaveModelBertCased=\"BertCased\"\n",
        "\n",
        "adressToSaveModelSciBertUncased=\"ScibertUncased\"\n",
        "adressToSaveModelSciBertCased=\"ScibertCased\"\n",
        "\n",
        "RobertaTokenizer=RobertaTokenizer\n",
        "RobertaModel=RobertaForSequenceClassification\n",
        "RobertaString=\"roberta-base\"\n",
        "adressToSaveModelRoberta=\"RobertaCased\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrniuIM4AhJr"
      },
      "source": [
        "  label_dict = {}\n",
        "  label_dict[\"intro\"]=0\n",
        "  label_dict[\"related\"]=1\n",
        "  label_dict[\"method\"]=2\n",
        "  label_dict[\"results\"]=3\n",
        "  label_dict[\"discussion\"]=4"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1CcvgUW5WSF"
      },
      "source": [
        "Get evaluation data, the finetuned Bert, Scibert and ROberta models from Dropbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEUpSBlK30c3",
        "outputId": "a91e1eb3-463a-4686-c217-dc4d9ee62b79"
      },
      "source": [
        "!wget -O bertmodel https://www.dropbox.com/s/d7qhpm3u9d602hp/BertCased5.model?dl=0\n",
        "!wget -O scibertmodel https://www.dropbox.com/s/rruwth9f9jtuhre/ScibertCased5.model?dl=0\n",
        "!wget -O robertamodel https://www.dropbox.com/s/ewjaccflj1elxo4/Roberta.model?dl=0\n",
        "!wget -O evaldata https://www.dropbox.com/s/gbn80vxzgrrtylz/EvaluationDataUnarxive.json?dl=0\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-05 19:21:27--  https://www.dropbox.com/s/d7qhpm3u9d602hp/BertCased5.model?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/d7qhpm3u9d602hp/BertCased5.model [following]\n",
            "--2021-09-05 19:21:28--  https://www.dropbox.com/s/raw/d7qhpm3u9d602hp/BertCased5.model\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com/cd/0/inline/BVnRn-Ps49OcYd2fZqhy2SwkASiUSbQlOCXI8Mg7LBIU-7QZq8tN_OUeySMisflEtB4SRlWGD8UCftfL5ldAtG2FMZ06vnpFb0NKkLVCZzKQbxGvaeg40P4EqK_UG1Vl2Lp-neG6DfisECF7LFScyhXR/file# [following]\n",
            "--2021-09-05 19:21:28--  https://uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com/cd/0/inline/BVnRn-Ps49OcYd2fZqhy2SwkASiUSbQlOCXI8Mg7LBIU-7QZq8tN_OUeySMisflEtB4SRlWGD8UCftfL5ldAtG2FMZ06vnpFb0NKkLVCZzKQbxGvaeg40P4EqK_UG1Vl2Lp-neG6DfisECF7LFScyhXR/file\n",
            "Resolving uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com (uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com (uc520fe8e258a22e23c48658447c.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433340589 (413M) [text/plain]\n",
            "Saving to: ‘bertmodel’\n",
            "\n",
            "bertmodel           100%[===================>] 413.27M  69.2MB/s    in 6.5s    \n",
            "\n",
            "2021-09-05 19:21:35 (63.1 MB/s) - ‘bertmodel’ saved [433340589/433340589]\n",
            "\n",
            "--2021-09-05 19:21:35--  https://www.dropbox.com/s/rruwth9f9jtuhre/ScibertCased5.model?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/rruwth9f9jtuhre/ScibertCased5.model [following]\n",
            "--2021-09-05 19:21:35--  https://www.dropbox.com/s/raw/rruwth9f9jtuhre/ScibertCased5.model\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com/cd/0/inline/BVkC7_Ld03mUweWpvARVD2UVPriODId3F-mV4o4JVWd3dpcn6tKvdl1GkRgQ3vbF6W-W8Pf4bxNEnLPAeQw2OIBMh0sXBegOPsi5nwZpThzcKtcx3VNT0UH71n2SG8yiQ1JeMNCvDyHDnMCW-uS8jwxx/file# [following]\n",
            "--2021-09-05 19:21:36--  https://ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com/cd/0/inline/BVkC7_Ld03mUweWpvARVD2UVPriODId3F-mV4o4JVWd3dpcn6tKvdl1GkRgQ3vbF6W-W8Pf4bxNEnLPAeQw2OIBMh0sXBegOPsi5nwZpThzcKtcx3VNT0UH71n2SG8yiQ1JeMNCvDyHDnMCW-uS8jwxx/file\n",
            "Resolving ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com (ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com (ucb8b85c317cef09822f5d3695ac.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 439853229 (419M) [text/plain]\n",
            "Saving to: ‘scibertmodel’\n",
            "\n",
            "scibertmodel        100%[===================>] 419.48M  11.8MB/s    in 24s     \n",
            "\n",
            "2021-09-05 19:22:00 (17.7 MB/s) - ‘scibertmodel’ saved [439853229/439853229]\n",
            "\n",
            "--2021-09-05 19:22:00--  https://www.dropbox.com/s/ewjaccflj1elxo4/Roberta.model?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ewjaccflj1elxo4/Roberta.model [following]\n",
            "--2021-09-05 19:22:01--  https://www.dropbox.com/s/raw/ewjaccflj1elxo4/Roberta.model\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com/cd/0/inline/BVlddykK1QCVTv6RSjb1iSy8-ldhGuZVr7_BiZjeAOCJYsX_RVO8oElhwpsIsOpHBC71GhLRk9daYkUezqFbfPtAHBeqhL2AXkzGxedpj8duaBSMZ8rfqpR_4FvBGwBIY3fYZlJqdahH7SRD3cggkXiH/file# [following]\n",
            "--2021-09-05 19:22:02--  https://uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com/cd/0/inline/BVlddykK1QCVTv6RSjb1iSy8-ldhGuZVr7_BiZjeAOCJYsX_RVO8oElhwpsIsOpHBC71GhLRk9daYkUezqFbfPtAHBeqhL2AXkzGxedpj8duaBSMZ8rfqpR_4FvBGwBIY3fYZlJqdahH7SRD3cggkXiH/file\n",
            "Resolving uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com (uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com (uc7e85fb99648921720cda191cc6.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498683309 (476M) [text/plain]\n",
            "Saving to: ‘robertamodel’\n",
            "\n",
            "robertamodel        100%[===================>] 475.58M  33.8MB/s    in 13s     \n",
            "\n",
            "2021-09-05 19:22:15 (36.3 MB/s) - ‘robertamodel’ saved [498683309/498683309]\n",
            "\n",
            "--2021-09-05 19:22:15--  https://www.dropbox.com/s/gbn80vxzgrrtylz/EvaluationDataUnarxive.json?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/gbn80vxzgrrtylz/EvaluationDataUnarxive.json [following]\n",
            "--2021-09-05 19:22:16--  https://www.dropbox.com/s/raw/gbn80vxzgrrtylz/EvaluationDataUnarxive.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com/cd/0/inline/BVnwQOasG35CywV4psLrtRAKkvqc1nh8PGs-ou52LlvRGVRZGJtp7XdU4ptRAjYj7LVu4zxW-MfaSmZzkaxDc0tbMaybsyUO3xfJ_6xU_W3oGFWVYWUG4XhxNm6UGxgERtj5YwyLk5_Z-8i3scvEjRaK/file# [following]\n",
            "--2021-09-05 19:22:16--  https://uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com/cd/0/inline/BVnwQOasG35CywV4psLrtRAKkvqc1nh8PGs-ou52LlvRGVRZGJtp7XdU4ptRAjYj7LVu4zxW-MfaSmZzkaxDc0tbMaybsyUO3xfJ_6xU_W3oGFWVYWUG4XhxNm6UGxgERtj5YwyLk5_Z-8i3scvEjRaK/file\n",
            "Resolving uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com (uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com (uc3ea611dec769ebb8b0b5545fa9.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191506 (187K) [text/plain]\n",
            "Saving to: ‘evaldata’\n",
            "\n",
            "evaldata            100%[===================>] 187.02K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-09-05 19:22:16 (1.77 MB/s) - ‘evaldata’ saved [191506/191506]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdilJj_cUYje",
        "outputId": "43b1dc62-f197-477b-e858-a85ed8fca9d6"
      },
      "source": [
        "filename=\"/content/evaldata\"\n",
        "with open(filename, 'r') as json_file:\n",
        "  data = json.load(json_file)\n",
        "array=evaluateIMRADFromJson(data, BertStringCased, BertModelName, \"/content/bertmodel\", BertTokenizerName, False, False)\n",
        "x=[]\n",
        "y=[]\n",
        "z=[]\n",
        "q=[]\n",
        "for a in array[0]:\n",
        "  for b in a:\n",
        "    x.append(b)\n",
        "for a in array[1]:\n",
        "  for b in a:\n",
        "    y.append(b)\n",
        "for a in array[2]:\n",
        "  for b in a:\n",
        "    z.append(b)\n",
        "for a in array[3]:\n",
        "  for b in a:\n",
        "    q.append(b)\n",
        "#there will be a warning about the BERT weights, this is expected\n",
        "print(\"\\nMicro F1 Scores:\")\n",
        "print(f\"Uncased BERT {f1_score(x,y, average='micro')}\")\n",
        "print(f\"Uncased BERT with simple context {f1_score(x,z, average='micro')}\")\n",
        "print(f\"Uncased BERT with sections {f1_score(x,q, average='micro')}\")\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Micro F1 Scores:\n",
            "Uncased BERT 0.6394052044609665\n",
            "Uncased BERT with simple context 0.7211895910780669\n",
            "Uncased BERT with sections 0.8252788104089219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oijVatRsVD2x",
        "outputId": "8a03ebe3-a764-4b23-f78c-bd2a98d51574"
      },
      "source": [
        "array=evaluateIMRADFromJson(data, ScibertStringCased, AutoModelForSequenceClassification , \"/content/scibertmodel\", BertTokenizerName, False, False)\n",
        "x=[]\n",
        "y=[]\n",
        "z=[]\n",
        "q=[]\n",
        "v=[]\n",
        "for a in array[0]:\n",
        "  for b in a:\n",
        "    x.append(b)\n",
        "for a in array[1]:\n",
        "  for b in a:\n",
        "    y.append(b)\n",
        "for a in array[2]:\n",
        "  for b in a:\n",
        "    z.append(b)\n",
        "for a in array[3]:\n",
        "  for b in a:\n",
        "    q.append(b)\n",
        "\n",
        "print(\"\\nMicro F1 Scores:\")\n",
        "print(f\"Uncased SciBERT {f1_score(x,y, average='micro')}\")\n",
        "print(f\"Uncased SciBERT with simple context {f1_score(x,z, average='micro')}\")\n",
        "print(f\"Uncased SciBERT with sections {f1_score(x,q, average='micro')}\")\n",
        "\n",
        "print(\"\\nMicro F1 Scores, Recall and Precision for different categories for SciBERT with simple context:\")\n",
        "\n",
        "for i in label_dict:\n",
        "  v=z[:]\n",
        "  w=x[:]\n",
        "  for j in range (0, len(z)):\n",
        "    if x[j]!=i:\n",
        "      w[j]=\"False\"\n",
        "    if x[j]==i:\n",
        "      w[j]=i\n",
        "      \n",
        "    if z[j]!=i:\n",
        "      v[j]=\"False\"\n",
        "    if z[j]==i:\n",
        "      v[j]=i\n",
        "\n",
        "  print(f\"\\nSection: {i}\")\n",
        "  print(f\"F1 Score: {f1_score(w,v, pos_label=i,average='binary')}\")\n",
        "  print(f\"Recall: {recall_score(w,v,pos_label=i, average='binary')}\")\n",
        "  print(f\"Precision: {precision_score(w,v, pos_label=i,average='binary')}\")\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Micro F1 Scores:\n",
            "Uncased SciBERT 0.7211895910780669\n",
            "Uncased SciBERT with simple context 0.7881040892193308\n",
            "Uncased SciBERT with sections 0.9107806691449816\n",
            "\n",
            "Micro F1 Scores, Recall and Precision for different categories for SciBERT with simple context:\n",
            "\n",
            "Section: intro\n",
            "F1 Score: 0.7708333333333334\n",
            "Recall: 0.7254901960784313\n",
            "Precision: 0.8222222222222222\n",
            "\n",
            "Section: related\n",
            "F1 Score: 0.7962962962962964\n",
            "Recall: 0.8113207547169812\n",
            "Precision: 0.7818181818181819\n",
            "\n",
            "Section: method\n",
            "F1 Score: 0.8333333333333333\n",
            "Recall: 0.9016393442622951\n",
            "Precision: 0.7746478873239436\n",
            "\n",
            "Section: results\n",
            "F1 Score: 0.7619047619047619\n",
            "Recall: 0.7692307692307693\n",
            "Precision: 0.7547169811320755\n",
            "\n",
            "Section: discussion\n",
            "F1 Score: 0.7628865979381444\n",
            "Recall: 0.7115384615384616\n",
            "Precision: 0.8222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eozyMThofIa7",
        "outputId": "eee1539a-f04e-4682-a2e4-1e37aadf946f"
      },
      "source": [
        "array=evaluateIMRADFromJson(data, RobertaString, RobertaForSequenceClassification, \"/content/robertamodel\", RobertaTokenizer, True, True)\n",
        "\n",
        "x=[]\n",
        "y=[]\n",
        "z=[]\n",
        "q=[]\n",
        "for a in array[0]:\n",
        "  for b in a:\n",
        "    x.append(b)\n",
        "for a in array[1]:\n",
        "  for b in a:\n",
        "    y.append(b)\n",
        "for a in array[2]:\n",
        "  for b in a:\n",
        "    z.append(b)\n",
        "for a in array[3]:\n",
        "  for b in a:\n",
        "    q.append(b)\n",
        "\n",
        "print(\"\\nMicro F1 Scores:\")\n",
        "print(f\"Uncased RoBERTa {f1_score(x,y, average='micro')}\")\n",
        "print(f\"Uncased RoBERTa with simple context {f1_score(x,z, average='micro')}\")\n",
        "print(f\"Uncased RoBERTa with sections {f1_score(x,q, average='micro')}\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roberta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Micro F1 Scores:\n",
            "Uncased RoBERTa 0.6840148698884758\n",
            "Uncased RoBERTa with simple context 0.758364312267658\n",
            "Uncased RoBERTa with sections 0.9479553903345725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXUuBo4XCsl8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}